{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import time \n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta base\n",
    "BASE_PATH = \"../data/processed/Excels/\"\n",
    "\n",
    "# Definir las rutas de los archivos de entrenamiento y prueba\n",
    "TRAIN_PATHS = [\n",
    "    \"X_train_con_outliers.xlsx\",\n",
    "    \"X_train_sin_outliers.xlsx\",\n",
    "    \"X_train_sel_k4.xlsx\"\n",
    "]\n",
    "\n",
    "TEST_PATHS = [\n",
    "    \"X_test_con_outliers.xlsx\",\n",
    "    \"X_test_sin_outliers.xlsx\",\n",
    "    \"X_test_sel_k4.xlsx\"\n",
    "]\n",
    "\n",
    "# Leer los datasets de entrenamiento\n",
    "TRAIN_DATASETS = [pd.read_excel(os.path.join(BASE_PATH, path)) for path in TRAIN_PATHS]  # X_train \n",
    "\n",
    "# Leer los datasets de prueba\n",
    "TEST_DATASETS = [pd.read_excel(os.path.join(BASE_PATH, path)) for path in TEST_PATHS]  # X_test\n",
    "\n",
    "# Leer las etiquetas (target) de entrenamiento y prueba\n",
    "y_train = pd.read_excel(os.path.join(BASE_PATH, \"y_train.xlsx\")).values.ravel()\n",
    "y_test = pd.read_excel(os.path.join(BASE_PATH, \"y_test.xlsx\")).values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_selected_features = {}\n",
    "# model_results = []\n",
    "# k = 6\n",
    "\n",
    "# for index, dataset in enumerate(TRAIN_DATASETS):\n",
    "#     print(f\"Procesando dataset {index + 1}\")\n",
    "\n",
    "#     # Selección de características usando SelectKBest con f_classif (para clasificación)\n",
    "#     selector = SelectKBest(f_classif, k=k)\n",
    "#     X_train = selector.fit_transform(dataset, y_train)  # Ajuste y transformación para el conjunto de entrenamiento\n",
    "#     X_test = selector.transform(TEST_DATASETS[index])  # Transformación para el conjunto de prueba\n",
    "\n",
    "#     # Almacenar las características seleccionadas en el diccionario\n",
    "#     selected_features = dataset.columns[selector.get_support()].tolist()  # Obtener los nombres de las características seleccionadas\n",
    "#     all_selected_features[TRAIN_PATHS[index]] = selected_features  # Usar el nombre del archivo como clave\n",
    "\n",
    "#     ### ENTRENAR MODELO ###\n",
    "\n",
    "#     # Crear y entrenar el modelo de regresión logística (clasificación)\n",
    "#     model = XGBClassifier(random_state = 95)  ### Asegúrate de ajustar los parámetros según sea necesario!!! \n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     # Predicciones en los datos de entrenamiento y prueba\n",
    "#     y_train_pred = model.predict(X_train)\n",
    "#     y_test_pred = model.predict(X_test)\n",
    "\n",
    "#     # Calcular la precisión (accuracy) para los datos de entrenamiento\n",
    "#     accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "#     precision_train = precision_score(y_train, y_train_pred, average='weighted')  # Asegúrate de ajustar el 'average'\n",
    "#     recall_train = recall_score(y_train, y_train_pred, average='weighted')\n",
    "#     f1_train = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "#     # Calcular la precisión (accuracy) para los datos de prueba\n",
    "#     accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "#     precision_test = precision_score(y_test, y_test_pred, average='weighted')\n",
    "#     recall_test = recall_score(y_test, y_test_pred, average='weighted')\n",
    "#     f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "#     # Almacenar los resultados en la lista, usando el nombre del archivo como \"dataset\"\n",
    "#     model_results.append(\n",
    "#     {\n",
    "#         \"dataset_TRAIN\": TRAIN_PATHS[index],  \n",
    "#         \"dataset_TEST\": TEST_PATHS[index],          \n",
    "#         \"accuracy_train\": accuracy_train,  \n",
    "#         \"accuracy_test\": accuracy_test,          \n",
    "#         \"precision_train\": precision_train, \n",
    "#         \"precision_test\": precision_test,          \n",
    "#         \"recall_train\": recall_train,  \n",
    "#         \"recall_test\": recall_test,          \n",
    "#         \"f1_train\": f1_train,  \n",
    "#         \"f1_test\": f1_test,  \n",
    "#     }\n",
    "# )\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando dataset 1\n",
      "Procesando dataset 2\n",
      "Procesando dataset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/sklearn/feature_selection/_univariate_selection.py:783: UserWarning: k=6 is greater than n_features=4. All the features will be returned.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados para el dataset: X_train_con_outliers.xlsx\n",
      "Accuracy Train: 1.0\n",
      "Accuracy Test: 0.7727272727272727\n",
      "Precision Train: 1.0\n",
      "Precision Test: 0.7681086064462481\n",
      "Recall Train: 1.0\n",
      "Recall Test: 0.7727272727272727\n",
      "F1 Train: 1.0\n",
      "F1 Test: 0.7686942607304208\n",
      "\n",
      "Resultados para el dataset: X_train_sin_outliers.xlsx\n",
      "Accuracy Train: 1.0\n",
      "Accuracy Test: 0.7402597402597403\n",
      "Precision Train: 1.0\n",
      "Precision Test: 0.7364035964035964\n",
      "Recall Train: 1.0\n",
      "Recall Test: 0.7402597402597403\n",
      "F1 Train: 1.0\n",
      "F1 Test: 0.7378112083994437\n",
      "\n",
      "Resultados para el dataset: X_train_sel_k4.xlsx\n",
      "Accuracy Train: 1.0\n",
      "Accuracy Test: 0.7467532467532467\n",
      "Precision Train: 1.0\n",
      "Precision Test: 0.7457112288048213\n",
      "Recall Train: 1.0\n",
      "Recall Test: 0.7467532467532467\n",
      "F1 Train: 1.0\n",
      "F1 Test: 0.746199817757915\n"
     ]
    }
   ],
   "source": [
    "# Ruta base\n",
    "BASE_PATH = \"../data/processed/Excels/\"\n",
    "\n",
    "# Definir las rutas de los archivos de entrenamiento y prueba\n",
    "TRAIN_PATHS = [\n",
    "    \"X_train_con_outliers.xlsx\",\n",
    "    \"X_train_sin_outliers.xlsx\",\n",
    "    \"X_train_sel_k4.xlsx\"\n",
    "]\n",
    "\n",
    "TEST_PATHS = [\n",
    "    \"X_test_con_outliers.xlsx\",\n",
    "    \"X_test_sin_outliers.xlsx\",\n",
    "    \"X_test_sel_k4.xlsx\"\n",
    "]\n",
    "\n",
    "# Leer los datasets de entrenamiento\n",
    "TRAIN_DATASETS = [pd.read_excel(os.path.join(BASE_PATH, path)) for path in TRAIN_PATHS]  # X_train \n",
    "\n",
    "# Leer los datasets de prueba\n",
    "TEST_DATASETS = [pd.read_excel(os.path.join(BASE_PATH, path)) for path in TEST_PATHS]  # X_test\n",
    "\n",
    "# Leer las etiquetas (target) de entrenamiento y prueba\n",
    "y_train = pd.read_excel(os.path.join(BASE_PATH, \"y_train.xlsx\")).values.ravel()\n",
    "y_test = pd.read_excel(os.path.join(BASE_PATH, \"y_test.xlsx\")).values.ravel()\n",
    "\n",
    "all_selected_features = {}\n",
    "model_results = []\n",
    "k = 6  # Número de características a seleccionar\n",
    "\n",
    "for index, dataset in enumerate(TRAIN_DATASETS):\n",
    "    print(f\"Procesando dataset {index + 1}\")\n",
    "\n",
    "    # Selección de características usando SelectKBest con f_classif (para clasificación)\n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    X_train = selector.fit_transform(dataset, y_train)  # Ajuste y transformación para el conjunto de entrenamiento\n",
    "    X_test = selector.transform(TEST_DATASETS[index])  # Transformación para el conjunto de prueba\n",
    "\n",
    "    # Almacenar las características seleccionadas en el diccionario\n",
    "    selected_features = dataset.columns[selector.get_support()].tolist()  # Obtener los nombres de las características seleccionadas\n",
    "    all_selected_features[TRAIN_PATHS[index]] = selected_features  # Usar el nombre del archivo como clave\n",
    "\n",
    "    ### ENTRENAR MODELO ###\n",
    "\n",
    "    # Crear y entrenar el modelo XGBClassifier\n",
    "    model = XGBClassifier(random_state=95)  # Ajustar los parámetros según sea necesario\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicciones en los datos de entrenamiento y prueba\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcular la precisión (accuracy) para los datos de entrenamiento\n",
    "    accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "    precision_train = precision_score(y_train, y_train_pred, average='weighted')  # Ajustar 'average'\n",
    "    recall_train = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    f1_train = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "    # Calcular la precisión (accuracy) para los datos de prueba\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    precision_test = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    recall_test = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    # Almacenar los resultados en la lista, usando el nombre del archivo como \"dataset\"\n",
    "    model_results.append(\n",
    "    {\n",
    "        \"dataset_TRAIN\": TRAIN_PATHS[index],  \n",
    "        \"dataset_TEST\": TEST_PATHS[index],          \n",
    "        \"accuracy_train\": accuracy_train,  \n",
    "        \"accuracy_test\": accuracy_test,          \n",
    "        \"precision_train\": precision_train, \n",
    "        \"precision_test\": precision_test,          \n",
    "        \"recall_train\": recall_train,  \n",
    "        \"recall_test\": recall_test,          \n",
    "        \"f1_train\": f1_train,  \n",
    "        \"f1_test\": f1_test,  \n",
    "    }\n",
    ")\n",
    "# Guardar los resultados en un archivo JSON\n",
    "with open(\"../models/model_results.json\", \"w\") as json_file:\n",
    "    json.dump(model_results, json_file, indent=4)\n",
    "\n",
    "# Guardar las características seleccionadas en un archivo JSON\n",
    "with open(\"../models/selected_features.json\", \"w\") as json_file:\n",
    "    json.dump(all_selected_features, json_file, indent=4)\n",
    "\n",
    "# Mostrar los resultados\n",
    "for result in model_results:\n",
    "    print(f\"\\nResultados para el dataset: {result['dataset_TRAIN']}\")\n",
    "    print(f\"Accuracy Train: {result['accuracy_train']}\")\n",
    "    print(f\"Accuracy Test: {result['accuracy_test']}\")\n",
    "    print(f\"Precision Train: {result['precision_train']}\")\n",
    "    print(f\"Precision Test: {result['precision_test']}\")\n",
    "    print(f\"Recall Train: {result['recall_train']}\")\n",
    "    print(f\"Recall Test: {result['recall_test']}\")\n",
    "    print(f\"F1 Train: {result['f1_train']}\")\n",
    "    print(f\"F1 Test: {result['f1_test']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de todos los datasets con k: 6:\n",
      "            dataset_TRAIN             dataset_TEST  accuracy_train  accuracy_test  precision_train  precision_test  recall_train  recall_test  f1_train  f1_test\n",
      "X_train_con_outliers.xlsx X_test_con_outliers.xlsx             1.0       0.772727              1.0        0.768109           1.0     0.772727       1.0 0.768694\n",
      "X_train_sin_outliers.xlsx X_test_sin_outliers.xlsx             1.0       0.740260              1.0        0.736404           1.0     0.740260       1.0 0.737811\n",
      "      X_train_sel_k4.xlsx       X_test_sel_k4.xlsx             1.0       0.746753              1.0        0.745711           1.0     0.746753       1.0 0.746200\n",
      "\n",
      "El mejor dataset seleccionado según la mayor precisión en el conjunto de prueba:\n",
      "accuracy_train         1.0\n",
      "accuracy_test     0.772727\n",
      "Name: 0, dtype: object\n",
      "\n",
      "El mejor dataset de entrenamiento es: X_train_con_outliers.xlsx\n",
      "El mejor dataset de prueba es: X_test_con_outliers.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Guardar los resultados del modelo en un DataFrame\n",
    "df_results = pd.DataFrame(model_results)\n",
    "\n",
    "# Guardar los resultados en un archivo JSON\n",
    "result_json_path = os.path.join(\"../models/model_results_k_{k}.json\")\n",
    "with open(result_json_path, 'w') as json_file:\n",
    "    json.dump(model_results, json_file, indent=4)\n",
    "\n",
    "# Guardar las características seleccionadas en un solo archivo JSON\n",
    "features_json_path = os.path.join(f\"../models/selected_features_k_{k}.json\")\n",
    "with open(features_json_path, 'w') as json_file:\n",
    "    json.dump(all_selected_features, json_file, indent=4)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Resultados de todos los datasets con k: {k}:\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Selección del mejor dataset basado en la mayor precisión en el conjunto de prueba\n",
    "best_dataset_index = df_results[\"accuracy_test\"].idxmax()  \n",
    "best_dataset = df_results.iloc[best_dataset_index]\n",
    "\n",
    "print(\"\\nEl mejor dataset seleccionado según la mayor precisión en el conjunto de prueba:\")\n",
    "print(best_dataset[[\"accuracy_train\", \"accuracy_test\"]])\n",
    "\n",
    "print(f\"\\nEl mejor dataset de entrenamiento es: {TRAIN_PATHS[best_dataset_index]}\")\n",
    "print(f\"El mejor dataset de prueba es: {TEST_PATHS[best_dataset_index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 5 minutos\n",
      "Training Accuracy: 80.6189\n",
      "Test Accuracy: 77.2727\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Crear y entrenar el modelo XGBoost\n",
    "model = XGBClassifier(random_state=95)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Número de estimadores (árboles)\n",
    "    'max_depth': [3, 5, 7],  # Profundidad máxima de los árboles\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Tasa de aprendizaje (eta)\n",
    "    'subsample': [0.8, 0.9, 1.0],  # Fracción de muestras utilizadas para entrenar cada árbol\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],  # Fracción de características utilizadas por cada árbol\n",
    "    'min_child_weight': [1, 5, 10],  # Mínimo peso de las muestras en cada nodo hijo\n",
    "    'gamma': [0, 0.1, 0.2],  # Reducción de la pérdida para las particiones de los árboles\n",
    "    'scale_pos_weight': [1, 5],  # Ajuste de pesos para clases desbalanceadas\n",
    "    'max_delta_step': [0, 1],  # Paso máximo de delta para estabilizar el entrenamiento\n",
    "}\n",
    "\n",
    "# Configuración de GridSearchCV con validación cruzada estratificada\n",
    "grid = GridSearchCV(\n",
    "    model, \n",
    "    param_grid=param_grid, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),  # 5 pliegues\n",
    "    n_jobs=-1,  # Utilizar todos los núcleos disponibles\n",
    "    verbose=0,  # Cambiar verbose a 0 para suprimir la salida detallada\n",
    "    scoring='accuracy',  # Evaluar el modelo según la precisión\n",
    "    refit=True,  # Reajustar el modelo con los mejores parámetros encontrados\n",
    "    return_train_score=True,  # No devolver puntuaciones de entrenamiento\n",
    ")\n",
    "\n",
    "# Ajustar el modelo con el conjunto de entrenamiento filtrado\n",
    "grid.fit(X_train, y_train)\n",
    "final_model = grid.best_estimator_\n",
    "\n",
    "# Predicciones en los datos de entrenamiento y prueba\n",
    "y_pred_train = final_model.predict(X_train)\n",
    "y_pred_test = final_model.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "data_to_save = {\n",
    "    \"hyperparams\": grid.best_params_,\n",
    "    \"train_accuracy\": accuracy_score(y_train, y_pred_train),\n",
    "    \"test_accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "    \"train_predictions\": y_pred_train.tolist(),\n",
    "    \"test_predictions\": y_pred_test.tolist(),\n",
    "}\n",
    "\n",
    "# Guardamos los resultados en un archivo JSON\n",
    "with open(os.path.join(\"../models/hyperparams_XGB.json\"), \"w\") as json_file:\n",
    "    json.dump(data_to_save, json_file, indent=4)\n",
    "\n",
    "print(f\"Tiempo de ejecución: {execution_time / 60:.0f} minutos\")\n",
    "print(f\"Training Accuracy: {accuracy_score(y_train, y_pred_train) * 100:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_test) * 100:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7727272727272727\n",
      "Precision: 0.7111111111111111\n",
      "Recall: 0.5925925925925926\n",
      "F1 Score: 0.6464646464646465\n",
      "Confusion Matrix:\n",
      " [[87 13]\n",
      " [22 32]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_test)\n",
    "prec = precision_score(y_test, y_pred_test)\n",
    "rec = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
